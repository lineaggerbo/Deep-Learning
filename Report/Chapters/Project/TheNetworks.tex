%!TEX root = ../../Main.tex
\graphicspath{{Chapters/Project/}}
%-------------------------------------------------------------------------------

\section{Networks} % (fold)
\label{sec:networks}

The network from the tutorial described earlier in this report is reffered to
as the initial network. The orther networks which are tried out in this
project is based on this network with some changes in the architecture. All
networks are described in this section.

\subsection{The initial CNN} % (fold)
\label{sub:the_initial_network}

The initial network is a six layer network. The network consists of three
convolutional layers and three fully connected layers. The layers are as
follows:

\begin{enumerate}
	\item The first layer consists of a convolution with a kernel size of five, a
	stride of one and a padding of two. The activation function is ReLU. It
	also consists of max pooling with a kernel size of three and a stride of two.
	\item The second layer consists of the same three parts with the same values
	of strides and kernel sizes.
	\item The third layer also consists of three parts. The convolutional part
	and the ReLU is the same as in the earlier layers. But instead of using max
	pooling, average pooling is used.
	\item The fourth layer is a normal fully connected layer.
	\item The fifth layer is also a normal fully connected layer.
	\item The sixth layer is a fully coonected Softmax layer.
\end{enumerate}
For a detailed description of each type/part of the layers see
\autoref{sec:convolutional_network_architecture}.

The initial network architecture file called:
\verb|cifar10__quick_traintest.prototx| can be found in the examples folder
under cifar10 in Caffe\cite{caffe}. This file has been used as a starting point
for all our networks which are explaind later in this section. 

For running the network on a dataset it is also necessary to edit the
configuration file: \verb|cifar10_quick_solver.prototxt| which can be found in
the same folder as the network architecture file. In this file a number of
hyperparameters for the network are specified such as the number of iterations
(epochs) and the value of the momentum parameter. The hyberparameters influence
on the convolution networks are described in \autoref{sec:training_and_testing}.


The only hyperparameter changed in the different networks is the number of
iterations (epochs). 1 epoch is specified as when all images in a dataset has
been through the entire network once. In Caffe the number of epochs are
specified as interations where 4000 iterations is equivalent to 8 epochs. In
our networks the number of iterations used is 4000 and/or 8000 (8 and/or 16
epochs). The initial network were trained with both 4000 and 8000 iterations in
order to see the performance difference.  


\subsubsection{Result}

The results for the initial network using 4000 and 8000 iterations for training
is as follows: 

Test result with 4000 iterations (8 epochs) achive the best accuracy on 0.7227.   

Test result with 8000 iterations (16 epochs) achive an accuracy on 0.7191. 



% subsection the_original_network (end)
\subsection{Customized AlexNet} % (fold)
\label{sub:alexNet}

We have tried to mimic the architecture of AlexNet\cite{AlexNet} in order
to test its performance on the CIFAR-10 dataset. The original AlexNet consists of 8
layers and are using delineation of responsibilities between two GPUs. Since we
did not have multiple GPUs avaliable we focused on designing the network for a
single GPU as in \cite{ZeilerFergus}. 

AlexNet was trained, validated and tested on the ImageNet dataset which contain
much larger images than the CIFAR-10 dataset. This meant that we had to change
the kernel size, the stride and the pooling in order for our network to be
compatible with the CIFAR-10 images. 


Our AlexNet configuration for Caffe is a 8 layer network with five convolutional
layers and three fully connected layers. The layers are as follows:
\begin{enumerate}
	\item The first layer consists of a convolution with a kernel size of five, a
	stride of one and a padding of two. The activation function is ReLU and last in
	the layer max pooling is applied with a kernel size of three and a stride of
	two. 
	\item The second layer is equivalent to the first layer 
	\item The third layer is also equivalent to the first layer but without any
	pooling in it. 
	\item The fourth layer is equivalent to the third layer 
	\item The fifth layer is equivalent to the first layer with max pooling  
	\item The sixth layer is a normal fully connected layer
	\item The seventh layer is also a normal fully connected layer
	\item The eight layer is a fully coonected Softmax layer
\end{enumerate}

For a detailed description of each type/part of the layers see
\autoref{sec:convolutional_network_architecture}. 


The differences from the original AlexNet is the change in kernel size for both
convolution and for max pooling which is scaled down to fit the smaller
images in CIFAR-10. Also the stride is minimized to one in the convolutions but
not in the max pooling. Another argument for changing the value to smaller
numbers besides fitting the network to the dataset is as explained in
\cite{ZeilerFergus} where Zeiler and Fergus conclude that ``the overall depth of
the model is important for obtaining good performance''.  Zeiler and
Fergus\cite{ZeilerFergus} reduce the size of the kernel size from 11 to 7 and
the stride from 4 to 2 in the first layer and their reduction in the first layer
allows the network to retain more information which improved their overall
performance. 


Our hypophysis was that by introducing more layers into the network compared to
the initial network would improve the performance and thereby increase the
accuracy.


\subsubsection{Result}

The result for our AlexNet using 8000 iterations for training on the CIFAR-10
dataset is as follows: 

Test result with 8000 iterations (16 epochs) achive an accuracy on 0.6992. 


% subsection another_network (end)
\subsection{CNN with additional convolution layer} % (fold)
\label{sub:cnn}


Additional conv 4 layer u. max pool



Beskrivelse af arkitekturen og implementationsfilerne

Beskrivelse af resultaterne

\subsubsection{Results}

The result for our CNN using 4000 iterations for training on the CIFAR-10
dataset is as follows: 

Test result with 4000 iterations (8 epochs) achive an accuracy on 	0.7186.


% subsection cnn (end)
% section the_networks (end)