%!TEX root = ../../Main.tex
\graphicspath{{Chapters/Exercise2/}}
%-------------------------------------------------------------------------------

\chapter{Exercise 2}

\section{Neural Network layer extension} % (fold)
\label{sec:Neural_Network_layer_extension}

In the second exercise of the Deep Learning course we extend our Neural
network implementation from the first exercise with three modules. The modules
implemented are a Dropout module, a convolution module and a pooling layer
module. 

In order to make the network more robust and agile towards data variations we
implemented different types of data augmentation. The overall idea of using data
augmentation is to artificially introduce variations in the dataset. We have
implemented 4 types of augmentation: horizontal flip, random crops, change in
contrast and change in brightness. Another reason for doing data augmentation
is to reduce overfitting and we have tried to do this by combining the data
augmentation and dropout. 


The solution to exercise 2 can be found in the Appendix. 
 

% section section_name (end)